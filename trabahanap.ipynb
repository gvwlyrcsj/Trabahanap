{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import faiss\n",
    "import pickle\n",
    "from tqdm import tqdm \n",
    "from sklearn.metrics import precision_score  \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Define dataset path\n",
    "dataset_path = 'data/naukri_data_science_jobs_india.csv'\n",
    "\n",
    "# Check if dataset exists\n",
    "if not os.path.exists(dataset_path):\n",
    "    raise FileNotFoundError(f\"Dataset not found at {dataset_path}. Please ensure the file exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded and cleaned successfully with 12000 rows!\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Clean data: Replace NaN with empty strings and ensure all skills are strings\n",
    "data['Skills/Description'] = data['Skills/Description'].fillna('').astype(str)\n",
    "\n",
    "print(f\"Dataset loaded and cleaned successfully with {len(data)} rows!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating BERT embeddings: 100%|██████████| 12000/12000 [08:47<00:00, 22.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings for 12000 jobs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load BERT model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_bert_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Generate BERT embeddings for job descriptions\n",
    "data['embeddings'] = [\n",
    "    get_bert_embedding(row['Skills/Description'])\n",
    "    for _, row in tqdm(data.iterrows(), total=len(data), desc=\"Generating BERT embeddings\")\n",
    "]\n",
    "\n",
    "# Convert embeddings to NumPy array\n",
    "embeddings = np.array(data['embeddings'].tolist())\n",
    "print(f\"Generated embeddings for {len(embeddings)} jobs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings and metadata saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save embeddings and job metadata\n",
    "output_dir = 'models'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "embeddings_data = {\n",
    "    'jobs': data.to_dict(orient='records'),\n",
    "    'model_name': 'bert-base-uncased'\n",
    "}\n",
    "with open(os.path.join(output_dir, 'job_embeddings.pkl'), 'wb') as f:\n",
    "    pickle.dump(embeddings_data, f)\n",
    "\n",
    "print(\"Embeddings and metadata saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index contains 12000 embeddings.\n",
      "FAISS index saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Build FAISS index\n",
    "dimension = embeddings.shape[1]  \n",
    "index = faiss.IndexFlatL2(dimension)  \n",
    "index.add(np.array(embeddings, dtype='float32')) \n",
    "\n",
    "print(f\"FAISS index contains {index.ntotal} embeddings.\")\n",
    "\n",
    "# Save FAISS index\n",
    "faiss.write_index(index, os.path.join(output_dir, 'job_index.faiss'))\n",
    "print(\"FAISS index saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_jobs(user_skills, top_n=5):\n",
    "    user_embedding = get_bert_embedding(user_skills).reshape(1, -1)\n",
    "    \n",
    "    # Compute cosine similarities\n",
    "    similarities = cosine_similarity(user_embedding, embeddings)\n",
    "    \n",
    "    top_indices = similarities.argsort()[0][-top_n:][::-1]\n",
    "    \n",
    "    print(\"Top recommendations:\")\n",
    "    recommended_jobs = []\n",
    "    for idx in top_indices:\n",
    "        job_role = data.iloc[idx]['Job_Role']\n",
    "        similarity = similarities[0][idx]\n",
    "        print(f\"{len(recommended_jobs)+1}. {job_role} | Similarity: {similarity:.4f}\")\n",
    "        \n",
    "        recommended_jobs.append(job_role)\n",
    "    \n",
    "    return recommended_jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision(true_labels, predicted_labels):\n",
    "    return precision_score(true_labels, predicted_labels, average='micro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top recommendations:\n",
      "1. Data Engineer | Similarity: 0.9497\n",
      "2. Data Analyst - SAS + Python/SQL - 4-8yrs - Worked with big datasets | Similarity: 0.9439\n",
      "3. Data Scientist | Similarity: 0.9393\n",
      "4. Urgently Hiring Fresher For Data Engineer | Similarity: 0.9369\n",
      "5. Data Analyst | Similarity: 0.9346\n",
      "Recommendation precision: 0.8000\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "user_skills = \"Data engineering, Python, SQL, AWS\"\n",
    "recommended_jobs = recommend_jobs(user_skills)\n",
    "\n",
    "true_labels = [1, 0, 1, 0, 1]  \n",
    "predicted_labels = [1, 0, 1, 1, 1]  \n",
    "\n",
    "# Calculate precision for the recommendations (example values)\n",
    "precision = calculate_precision(true_labels, predicted_labels)\n",
    "print(f\"Recommendation precision: {precision:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top recommendations:\n",
      "1. Data Engineer | Similarity: 0.9497\n",
      "2. Data Analyst - SAS + Python/SQL - 4-8yrs - Worked with big datasets | Similarity: 0.9439\n",
      "3. Data Scientist | Similarity: 0.9393\n",
      "4. Urgently Hiring Fresher For Data Engineer | Similarity: 0.9369\n",
      "5. Data Analyst | Similarity: 0.9346\n",
      "Recommendation precision: 0.8000\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "user_skills = \"Data engineering, Python, SQL, AWS\"\n",
    "recommended_jobs = recommend_jobs(user_skills)\n",
    "\n",
    "true_labels = [1, 0, 1, 0, 1] \n",
    "predicted_labels = [1, 0, 1, 1, 1]  \n",
    "\n",
    "# Calculate precision for the recommendations (example values)\n",
    "precision = calculate_precision(true_labels, predicted_labels)\n",
    "print(f\"Recommendation precision: {precision:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
